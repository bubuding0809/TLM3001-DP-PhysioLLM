{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_LLM = 'llama3'\n",
    "CLOUD_LLM = 'llama3-70b-8192'\n",
    "VECTOR_DB_URL = 'http://localhost:6333'\n",
    "EMBEDDING_MODEL = 'NeuML/pubmedbert-base-embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.embeddings import sentence_transformer\n",
    "\n",
    "def get_vector_embeddings(embedding_model: str):\n",
    "    embeddings = sentence_transformer.SentenceTransformerEmbeddings(\n",
    "        model_name=embedding_model\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "print(\"Loading vector embeddings and creating Qdrant client...\")\n",
    "embeddings = get_vector_embeddings(EMBEDDING_MODEL)\n",
    "client = QdrantClient(VECTOR_DB_URL)\n",
    "db = Qdrant(\n",
    "    client=client,\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"test-collection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Create pydantic object for the grader result\n",
    "class GraderResult(BaseModel):\n",
    "    score: str = Field(\n",
    "        \"\", \n",
    "        description=\"A binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\n",
    "    )\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=GraderResult)\n",
    "\n",
    "RETRIEVAL_GRADER_PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are a grader assessing relevance of a retrieved document to a user question. \n",
    "If the document contains keywords related to the user question,\n",
    "grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "\n",
    "{format_instructions}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Here is the retrieved document:\n",
    "\n",
    "{document}\n",
    "\n",
    "Here is the user question: {question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatGroq(model=CLOUD_LLM)\n",
    "# llm = ChatOllama(model=LOCAL_LLM, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=RETRIEVAL_GRADER_PROMPT, \n",
    "    input_variables=[\"document\", \"question\"], \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "retrieval_grader = retrieval_grader.with_retry()\n",
    "\n",
    "query = \"When did sir Standford Raffles founded singapore?\"\n",
    "docs = db.similarity_search_with_score(query=query, k=5)\n",
    "\n",
    "# Add the query to the context\n",
    "for doc, score in docs:\n",
    "    result = retrieval_grader.invoke({\n",
    "        \"document\": doc,\n",
    "        \"question\": query\n",
    "    })\n",
    "    print(prompt.format(document=doc, question=query))\n",
    "    print(result, sep=\"\\n\\n\", end=\"\\n\\n\\n\" + \"-\" * 50 + \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatGroq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mPROMPT, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# llm = ChatOllama(model=LOCAL_LLM)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatGroq\u001b[49m(model\u001b[38;5;241m=\u001b[39mCLOUD_LLM)\n\u001b[1;32m     48\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     49\u001b[0m docs \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(query\u001b[38;5;241m=\u001b[39mquery, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatGroq' is not defined"
     ]
    }
   ],
   "source": [
    "### Generation Agent\n",
    "\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a expert in physiotherapy, you will be presented with a set of subjective and object assessment and your job is to come up with a well informed and researched differential diagnosis of the medical scenerio.\n",
    "Here are examples of the subjective and objective assessments and the expected differential diagnosis:\n",
    "\n",
    "Subjective Assessment:\n",
    "The patient, Mr. Smith, is a 45-year-old male who presents to the clinic with complaints of lower back pain that has been bothering him for the past two weeks. He describes the pain as dull and achy, located in the lumbar region, with occasional radiation down his left leg. He notes that the pain worsens with prolonged sitting or standing and is relieved by lying down. He denies any recent trauma or injury but mentions that he has a history of occasional low back pain, especially after heavy lifting or prolonged periods of inactivity. He rates the pain as a 6 out of 10 on the pain scale.On physical examination, Mr. Smith appears uncomfortable but is able to walk into the examination room without assistance.\n",
    "\n",
    "Objective Assessment:\n",
    "Vital signs are within normal limits. Inspection of the lumbar spine reveals no obvious deformities or asymmetry. Palpation elicits tenderness over the paraspinal muscles of the lumbar spine, particularly on the left side. Range of motion of the lumbar spine is mildly restricted, with pain on forward flexion and left lateral bending. Straight leg raise test is positive on the left side at 45 degrees, reproducing his symptoms of radiating pain down the left leg. Neurological examination reveals intact sensation and strength in the lower extremities, with no signs of motor weakness or sensory deficits. On physical examination, Mr. Smith appears uncomfortable but is able to walk into the examination room without assistance. Vital signs are within normal limits. Inspection of the lumbar spine reveals no obvious deformities or asymmetry. Palpation elicits tenderness over the paraspinal muscles of the lumbar spine, particularly on the left side. Range of motion of the lumbar spine is mildly restricted, with pain on forward flexion and left lateral bending. Straight leg raise test is positive on the left side at 45 degrees, reproducing his symptoms of radiating pain down the left leg. Neurological examination reveals intact sensation and strength in the lower extremities, with no signs of motor weakness or sensory deficits.\n",
    "\n",
    "Differential Diagnosis:\n",
    "Based on the assessments provided, my differential diagnosis for Mr. Smith would include:\n",
    "1. ...\n",
    "2. ...\n",
    "...\n",
    "\n",
    "IMPORTANT:\n",
    "- If the query is not related to physiotherapy or unrelated from the retrieved context, please answer with \"I am sorry, I am not able to answer this question.\"\n",
    "- Include citations and references to support your answer. <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|> \n",
    "\n",
    "Answer the question based only on the following context, this context should be used as source of ground truth to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: \n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "Given the following subjective and objective assessment, provide a well informed and researched differential diagnosis\n",
    "\n",
    "Subjective assessment:\n",
    "The patient, Mr. Smith, is a 45-year-old male who presents to the clinic with complaints of lower back pain that has been bothering him for the past two weeks. He describes the pain as dull and achy, located in the lumbar region, with occasional radiation down his left leg. He notes that the pain worsens with prolonged sitting or standing and is relieved by lying down. He denies any recent trauma or injury but mentions that he has a history of occasional low back pain, especially after heavy lifting or prolonged periods of inactivity. He rates the pain as a 6 out of 10 on the pain scale.\n",
    "\n",
    "Objective assessment:\n",
    "On physical examination, Mr. Smith appears uncomfortable but is able to walk into the examination room without assistance. Vital signs are within normal limits. Inspection of the lumbar spine reveals no obvious deformities or asymmetry. Palpation elicits tenderness over the paraspinal muscles of the lumbar spine, particularly on the left side. Range of motion of the lumbar spine is mildly restricted, with pain on forward flexion and left lateral bending. Straight leg raise test is positive on the left side at 45 degrees, reproducing his symptoms of radiating pain down the left leg. Neurological examination reveals intact sensation and strength in the lower extremities, with no signs of motor weakness or sensory deficits.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=PROMPT, input_variables=[\"context\", \"question\"])\n",
    "# llm = ChatOllama(model=LOCAL_LLM)\n",
    "llm = ChatGroq(model=CLOUD_LLM)\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "docs = db.similarity_search_with_score(query=query, k=5)\n",
    "\n",
    "def format_retrieved_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join([f\"metadata: {doc.metadata}\\nscore: {score}\\ncontent: {doc.page_content}\" for doc, score in docs])\n",
    "\n",
    "print(prompt.format(context=format_retrieved_docs(docs), question=query))\n",
    "print(\"Waiting for LLM response...\")\n",
    "stream = rag_chain.stream({\n",
    "  \"context\": format_retrieved_docs(docs),\n",
    "  \"question\": query\n",
    "})\n",
    "generation = \"\"\n",
    "for response in stream:\n",
    "    generation += response\n",
    "    print(response, flush=True, end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHallucinationGraderResult\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[1;32m      2\u001b[0m     score: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA binary score \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to indicate whether the answer is grounded in / supported by the facts provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mHallucinationGraderResult)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class HallucinationGraderResult(BaseModel):\n",
    "    score: str = Field(\n",
    "        \"\", \n",
    "        description=\"A binary score 'yes' or 'no' to indicate whether the answer is grounded in / supported by the facts provided.\"\n",
    "    )\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=HallucinationGraderResult)\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
    "an answer is grounded in / supported by as set of facts. Give a binary score of 'yes' or 'no' to indicate\n",
    "whether the answer is grounded in / supported by the facts provided.\n",
    "{format_instructions}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Here are the facts:\n",
    "\n",
    "----------------------------------------\n",
    "{facts}\n",
    "----------------------------------------\n",
    "\n",
    "Here is the answer: {answer}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT, \n",
    "    input_variables=[\"facts\", \"answer\"], \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "print(prompt.format(facts=format_retrieved_docs(docs), answer=generation))\n",
    "llm = ChatGroq(model=CLOUD_LLM)\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader = hallucination_grader.with_retry()\n",
    "\n",
    "hallucination_result = hallucination_grader.invoke({\n",
    "    \"facts\": format_retrieved_docs(docs),\n",
    "    \"answer\": generation\n",
    "})\n",
    "print(hallucination_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAnswerGraderResult\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[1;32m      2\u001b[0m     score: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA binary score \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to indicate whether the answer is useful to resolve the question\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mAnswerGraderResult)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "### Answer Grader Agent\n",
    "\n",
    "class AnswerGraderResult(BaseModel):\n",
    "    score: str = Field(\"\", description=\"A binary score 'yes' or 'no' to indicate whether the answer is useful to resolve the question\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=AnswerGraderResult)\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
    "an answer is useful to resolve a question. Give a binary score of 'yes' or 'no' to indicate whether the answer is useful to resolve the question.\n",
    "{format_instructions}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|> \n",
    "\n",
    "Here is the answer:\n",
    "----------------------------------------\n",
    "{answer}\n",
    "----------------------------------------\n",
    "\n",
    "Here is the question: \n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT, \n",
    "    input_variables=[\"answer\", \"question\"], \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "print(prompt.format(answer=generation, question=query))\n",
    "llm = ChatGroq(model=CLOUD_LLM)\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader = answer_grader.with_retry()\n",
    "\n",
    "answer_result = answer_grader.invoke({\n",
    "    \"answer\": generation,\n",
    "    \"question\": query\n",
    "})\n",
    "print(answer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonOutputParser\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# LLM\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[43mlocal_llm\u001b[49m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     11\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m question_router \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m JsonOutputParser()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_llm' is not defined"
     ]
    }
   ],
   "source": [
    "### Router Agent\n",
    "\n",
    "class RouterResult(BaseModel):\n",
    "    datasource: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"A binary score 'yes' or 'no' to indicate whether the question should be routed to the vectorstore or web search.\"\n",
    "    )\n",
    "parser = PydanticOutputParser(pydantic_object=RouterResult)\n",
    "\n",
    "\n",
    "llm = ChatGroq(model=LOCAL_LLM)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. \n",
    "    Use the vectorstore for questions on Muscle skeletal conditions, phyisotherapy domain and clinic evaluations.\n",
    "    You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search.\n",
    "    {format_instructions}<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|> \n",
    "    \n",
    "    Question to route: {question} <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "print(question_router.invoke({\"question\": question})) \n",
    "# Should return 'web_search' as the question is not related to physiotherapy or clinic evaluations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
